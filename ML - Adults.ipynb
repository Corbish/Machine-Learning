{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I begin importing all modules that I need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is from UCI. Its features are adults attributes like age, sex, etc wich will be used to determine the earning money for a person. The list of attributes are this:\n",
    "\n",
    "\n",
    " -age: continuous.\n",
    " \n",
    " -workclass: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.\n",
    " \n",
    " -fnlwgt: continuous.\n",
    " \n",
    " -education: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th,  Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.\n",
    " \n",
    " -education-num: continuous. Year of education\n",
    " \n",
    " -marital-status: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.\n",
    " \n",
    " -occupation: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners,  Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.\n",
    " \n",
    " -relationship: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.\n",
    " \n",
    " -race: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black.\n",
    " \n",
    " -sex: Female, Male.\n",
    " \n",
    " -capital-gain: continuous.\n",
    " \n",
    " -capital-loss: continuous.\n",
    " \n",
    " -hours-per-week: continuous.\n",
    " \n",
    " -native-country: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands.\n",
    " \n",
    " -annual salary: >50K, <=50k\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I load the data, wich is separate in train and test set. I prefer join this and make the split by myself, after analyse the features. Also I analyse duplicate values, and drop these. I do the same for NaN values, wich are represented with '?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Data\n",
    "#Data has not columns name, I put this in each set\n",
    "columns = ['age','workclass','fnlwgt','education','education-num', 'marital-status', 'occupation', 'relationship',\n",
    "          'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'Money per year']\n",
    "\n",
    "train = pd.read_csv('adult.data')\n",
    "train.set_axis(columns, axis='columns', inplace = True )\n",
    "test = pd.read_csv('adult_test.data')\n",
    "test.set_axis(columns, axis='columns', inplace = True )\n",
    "\n",
    "#I concatenate the two sets\n",
    "data = pd.concat([train, test], axis=0)\n",
    "data.reset_index(inplace=True)\n",
    "del data['index']\n",
    "#In the data set, some values has a point at the end, i drop this\n",
    "def drop_point(elem):\n",
    "    if elem[-1] == '.':\n",
    "        return elem[:-1]\n",
    "    else:\n",
    "        return elem\n",
    "data['Money per year'] = data['Money per year'].apply(drop_point)\n",
    "\n",
    "#data.duplicated().sum()\n",
    "data.drop_duplicates(keep='first', inplace=True)\n",
    "\n",
    "data.replace(' ?', np.nan, inplace=True)\n",
    "\n",
    "data.isnull().sum()\n",
    "data.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this little preprocesing, I split data in test and train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns.remove('Money per year')\n",
    "X = data[columns]\n",
    "y = data['Money per year']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "train = pd.concat([X_train, y_train], axis=1)\n",
    "test = pd.concat([X_test, y_test], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I proced to visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# values = pd.melt(train, value_vars = train.columns.to_list())\n",
    "# g = sns.FacetGrid(values, col='variable', col_wrap=2, height=7, aspect=1, sharex=False, sharey=False)\n",
    "# g.map(sns.histplot, 'value')\n",
    "# g.set_xticklabels(rotation=90)\n",
    "# g.fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREPROCESING\n",
    "\n",
    "### AGE\n",
    "\n",
    "I group the age distribution in 15 bins, Then I apply this grouped criteria to the respective column. In this way, I also categorize each age interval with a kind of label encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist, div = np.histogram(train['age'], bins = 15)\n",
    "div = np.around(div)\n",
    "div[-1] = 91\n",
    "\n",
    "train['age'] = np.digitize(train['age'], bins=div)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "ages = []\n",
    "for i in range(len(div) - 1):\n",
    "    a = div[i]\n",
    "    b = div[i + 1] - 1\n",
    "    ages.append('['+str(a)+' , '+str(b)+']')\n",
    "ages[-1] = '[85.0 , 91.0]'\n",
    "age_pair = zip(list(range(1,16)), ages)\n",
    "age_pair =  dict(age_pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDUCATION\n",
    " I analyze the education feature. I see the histogram distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "def graph_hist_ratio(val):\n",
    "    sns.set(rc={\"figure.figsize\":(20, 10), \"xtick.direction\":'in'})\n",
    "    g = sns.histplot(train, x=val, hue='Money per year', multiple='layer')\n",
    "    plt.xticks(rotation=90)\n",
    "\n",
    "# graph_hist_ratio('education')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I group that class with similar proportion of money per year:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ratio_values(val):\n",
    "    mpy= train[[val, 'Money per year']]\n",
    "    more_50k = mpy[mpy['Money per year'] == ' >50K']\n",
    "    less_50k =  mpy[mpy['Money per year'] == ' <=50K']\n",
    "    less = less_50k.groupby(val).count()\n",
    "    more = more_50k.groupby(val).count()\n",
    "    ratio = more/less #rate between money per year great than 50K and less than 50K\n",
    "    ratio['index'] = list(range(len(ratio)))\n",
    "    return ratio.sort_values(by='Money per year')\n",
    "\n",
    "a = ratio_values('education').index\n",
    "a = np.array(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def educ_convert(value):\n",
    "    index = a\n",
    "    incom = index[[0,1,2,3,4,5,6,13]]\n",
    "    asoc = index[[7,8]]\n",
    "    maj = index[[10,14]]\n",
    "    college = index[[11,15]]\n",
    "    if value in incom:\n",
    "        return 'Incomplete HS'\n",
    "    elif value in asoc:\n",
    "        return 'Assoc'\n",
    "    elif value in maj:\n",
    "        return 'Dr/Prof'\n",
    "    elif value in college:\n",
    "        return 'College/Hs grad'\n",
    "    else:\n",
    "        return value\n",
    "\n",
    "train['education'] = train['education'].apply(educ_convert)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'education - num' features is the total education time in years. I can drop this, because is a redundant information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MARITAL STATUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph_hist_ratio('marital-status')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separated and widowed has the same proportion. I assume that I can define just a unique value 'Separated/Widowed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def status_convert(val):\n",
    "    if val in [' Separated', ' Widowed']:\n",
    "        return 'Separated/Widowed'\n",
    "    else:\n",
    "        return val\n",
    "\n",
    "train['marital-status'] = train['marital-status'].apply(status_convert) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WORKCLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph_hist_ratio('workclass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def workclass_convert(val):\n",
    "    if val in [' Never-worked', ' Without-pay']:\n",
    "        return 'No incomes'\n",
    "    elif val in [' Self-emp-not-inc', ' Local-gov']:\n",
    "        return 'Not-inc/Local-gov'\n",
    "    else:\n",
    "        return val\n",
    "\n",
    "train['workclass'] = train['workclass'].apply(workclass_convert) \n",
    "# point biserial correlation \n",
    "#  Chi-square estimate "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OCCUPATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# graph_hist_ratio('occupation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratio_values('occupation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def occupation_convert(val):\n",
    "    if val in [' Other-service', ' Handlers-cleaners']:\n",
    "        return 'Handlers and other services'\n",
    "    elif val in [' Machine-op-inspct', ' Adm-clerical', ' Farming-fishing']:\n",
    "        return 'Farming, clerical, op machine'\n",
    "    else:\n",
    "        return val\n",
    "train['occupation'] = train['occupation'].apply(occupation_convert) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RELATIONSHIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph_hist_ratio('relationship')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's seem there is nothing to do here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph_hist_ratio('race')\n",
    "# ratio_values('race')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def race_convert(val):\n",
    "    if val in [' Amer-Indian-Eskimo', ' Black', ' Other']:\n",
    "        return 'Minorities'\n",
    "    else:\n",
    "        return val\n",
    "train['race'] = train['race'].apply(race_convert) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CAPITAL LOSS & CAPITAL GAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph_hist_ratio('capital-loss')\n",
    "# graph_hist_ratio('capital-gain')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the histogram show a practically a unique value, I suppouse That these features can be droped. Previously, I check the histogram in a lower range, but the result is the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "da = train[train['capital-gain']<500]\n",
    "#sns.histplot(da, x ='capital-gain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train['capital-gain']\n",
    "del train['capital-loss']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CORRELATION\n",
    "Now, I analyze correlation between variables. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
